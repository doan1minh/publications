\documentclass[a4paper]{llncs}

%% There is a package that you should include first: nag. Why first? Because it warns you about
%% obsolete commands and package. https://daniel-j-h.github.io/post/latex-a-modern-approach/
\usepackage[l2tabu,orthodox]{nag}

%% TYPOGRAPHY

\usepackage{microtype}
% uncomment to enable palatino
% \usepackage{palatino,mathpazo}
\renewcommand{\tabcolsep}{1em}
\renewcommand*{\arraystretch}{1.2}
\setlength{\arraycolsep}{3pt}
\usepackage{inconsolata}

%% PAGE LAYOUT

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% \usepackage{lineno}
% \linenumbers{}

\pagestyle{plain}

%% TODO NOTES

\usepackage{xcolor}
\definecolor{oxygenorange}{HTML}{FFDD00}
\usepackage[color=oxygenorange]{todonotes}
\newcommand{\malb}[2][inline]{\todo[#1]{\textbf{malb:} #2}\xspace}

%% ANONYMOUS SUBMISSIONS

\def\isanonymous{0}

\usepackage{ifthen}
\newcommand{\anonymous}[2]{
\ifthenelse{\equal{\isanonymous}{1}}
{{#1}}
{{#2}}
}

%% MATHEMATICAL SYMBOLS

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{xspace}

\input{utf-8-macros}
\input{lwe-macros}

%% CRYPTO Notation

\usepackage[]{algorithm2e}
\usepackage[lambda,landau,operators,probability,sets,logic,complexity,asymptotics]{cryptocode}

%% OTHER PACKAGES

\usepackage{booktabs}  %% tables
\usepackage{comment}
\usepackage{embedfile}
\usepackage{enumitem}
\usepackage{url}

%% PLOTS

\usepackage{tikz,pgfplots}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}

% from pgfplotsthemetol.sty
\definecolor{DarkPurple}{HTML}{332288}
\definecolor{DarkBlue}{HTML}{6699CC}
\definecolor{LightBlue}{HTML}{88CCEE}
\definecolor{DarkGreen}{HTML}{117733}
\definecolor{DarkRed}{HTML}{661100}
\definecolor{LightRed}{HTML}{CC6677}
\definecolor{LightPink}{HTML}{AA4466}
\definecolor{DarkPink}{HTML}{882255}
\definecolor{LightPurple}{HTML}{AA4499}

\definecolor{DarkBrown}{HTML}{604c38}
\definecolor{DarkTeal}{HTML}{23373b}
\definecolor{LightBrown}{HTML}{EB811B}
\definecolor{LightGreen}{HTML}{14B03D}

%% LISTINGS

\usepackage{listings}
\lstdefinelanguage{Sage}[]{Python}{morekeywords={True,False,sage,cdef,cpdef,ctypedef,self},sensitive=true}
\lstset{frame=none,
          showtabs=False,
          showspaces=False,
          showstringspaces=False,
          commentstyle=\color{gray!80!black},
          keywordstyle={\color{gray!80!black}\textbf},
          stringstyle ={\color{gray!80!black}},
          basicstyle=\tt\small\relax,
          }

%% ALGORITHMS

\newcommand{\MYALG}[1][blank]{\ifthenelse{\equal{#1}{blank}}{\textsc{Silke}}{\textsc{Silke}\ensuremath{_{#1}}}\xspace}

%% OPENING

\title{On dual lattice attacks against small-secret LWE and parameter choices in HElib and SEAL}
\anonymous{}{
  \author{Martin R. Albrecht\thanks{This research was supported by EPSRC grants EP/L018543/1 ``Multilinear Maps in Cryptography'' and EP/P009417/1 ``Bit Security of Learning with Errors for Post-Quantum Cryptography and Fully Homomorphic Encryption''.}}
  \institute{Information Security Group\\
    Royal Holloway, University of London, Egham, Surrey TW20 0EX, UK \\
    \email{martin.albrecht@royalholloway.ac.uk}
  }
}


\begin{document}



\maketitle

\begin{abstract}
  We present novel variants of the dual-lattice attack against LWE in the presence of an unusually short secret. These variants are informed by recent progress in BKW-style algorithms for solving LWE\@. Applying them to parameter sets suggested by the homomorphic encryption libraries HElib and SEAL~v2.0 yields revised security estimates. Our techniques scale the exponent of the dual-lattice attack by a factor of \((2\,L)/(2\,L+1)\) when \(\log q = \Theta{\left(L \log n\right)}\), when the secret has constant hamming weight \(h\) and where \(L\) is the maximum depth of supported circuits. They also allow to half the dimension of the lattice under consideration at a multiplicative cost of \(2^{h}\) operations. Moreover, our techniques yield revised concrete security estimates. For example, both libraries promise 80 bits of security for LWE instances with $n=1024$ and $\log_2 q ≈ {47}$, while the techniques described in this work lead to estimated costs of 68 bits (SEAL~v2.0) and 62 bits (HElib).
\end{abstract}

\section{Introduction}\label{sec:introduction}

Learning with Errors (LWE), defined in Definition~\ref{def:lwe}, has proven to be a rich source of cryptographic constructions, from public-key encryption and Diffie-Hellman-style key exchange (cf.~\cite{JACM:Regev09,TCC:Peikert09,EC:LyuPeiReg10,EPRINT:DinXieLin12,SP:BCNS15,USENIX:ADPS16,CCS:BCDMNN16}) to fully homomorphic encryption (cf.~\cite{FOCS:BraVai11,ITCS:BraGenVai12,C:Brakerski12,EPRINT:FanVer12,C:GenSahWat13,EC:CheSte15}). 

\begin{definition}[LWE~\cite{JACM:Regev09}]\label{def:lwe}
  Let \(n,\,q\) be positive integers, χ be a probability distribution on $\ZZ$ and $\s$ be a secret vector in \(\ZZq^n\). We denote by \Ldis{} the probability distribution on \(\ZZq^n × \ZZq\) obtained by choosing \(\a ∈ \ZZq^n\) uniformly at random, choosing \(e ∈ \ZZ\) according to χ and considering it in \(\ZZq\), and returning  \((\a,c) = (\a,\Angle{\a,\s}+ e) ∈ \ZZq^n × \ZZq\).\\ 
  \emph{Decision-LWE} is the problem of deciding whether pairs \((\a, c)∈ \ZZq^n × \ZZq\) are sampled according to \(\Ldis\) or the uniform distribution on \(\ZZq^n × \ZZq\).\\
  \emph{Search-LWE} is the problem of recovering \(\s\) from \((\a, c)=(\a,\Angle{\a,\s} + e) ∈ \ZZq^n × \ZZq\) sampled according to \(\Ldis\).
\end{definition}

We may write LWE instances in matrix form $\left(\vec{A},\vec{c}\right)$, where rows correspond to samples $\left(\vec{a}_i,c_i\right)$. In many instantiations, $χ$ is a discrete Gaussian distribution with standard deviation $α\, q /\sqrt{2π}$. Though, in this work, like in many works on cryptanalysis of LWE, the details of the error distribution do not matter as long as we can bound the size of the error under additions.

The bit-security of concrete LWE instances is a prominent area of current cryptographic research, in particular in light of standardisation initiatives for LWE-based schemes and LWE-based (somewhat) homomorphic encryption being proposed for applications such as computation with medical data~\cite{KimLau15}. See~\cite{JMC:AlbPlaSco15} for a relatively recent survey of known (classical) attacks.

Applications such as~\cite{KimLau15} are enabled by progress in homomorphic encryption in recent years. The two most well-known homomorphic encryption libraries are HElib and SEAL\@. HElib~\cite{C:GenHalSma12,C:HalSho14} implements BGV~\cite{ITCS:BraGenVai12}. SEAL~v2.0~\cite{LaiPla16} implements FV~\cite{C:Brakerski12,EPRINT:FanVer12}. Both schemes fundamentally rely on the security of LWE\@.

However, results on the expected cost of solving generic LWE instances do not directly translate to LWE instances as used in fully homomorphic encryption (FHE). Firstly, because these instances are typically related to the Ring-LWE assumption~\cite{EC:LyuPeiReg10,EPRINT:LyuPeiReg13} instead of plain LWE\@. Secondly, because these instances are typically \emph{small-secret} instances. In particular,  they typically sample the secret $\s$ from some distribution $\B{}$ as defined below. We call such instances \(\B{}\)-secret LWE instances.

\begin{definition}\label{def:binary-secret}
  Let \(n, q\) be positive integers. We call 
  \begin{description}
    \item[$\B{}$] any distribution on \(\ZZq^n\) where each component $≤1$ in absolute value, i.e. $\|\s[i]\| ≤ 1$ for $\s \sample \B{}$.
    \item[$\B{+}$] the distribution on \(\ZZq^n\) where each component is independently sampled uniformly at random from \(\{0,1\}\).
    \item[$\B{-}$] the distribution on \(\ZZq^n\) where each component is independently sampled uniformly at random from \(\{-1,0,1\}\).  
    \item[$\mathcal{B}^{+}_{h}$] the distribution on \(\ZZq^n\) where components are sampled independently uniformly at random from \(\{0,1\}\) with the additional guarantee that at most \(h\) components are non-zero. 
    \item[$\mathcal{B}^{-}_{h}$] the distribution on \(\ZZq^n\) where components are sampled independently uniformly at random from \(\{-1,0,1\}\) with the additional guarantee that at most \(h\) components are non-zero. 
  \end{description}
\end{definition}

\begin{remark}
  In~\cite{STOC:BLPRS13}, instances with \(\s \sample \B{+}\) are referred to as binary-secret; \(\B{+}\) is used in~\cite{EPRINT:FanVer12}; \(\B{-}\) is used in Microsoft's SEAL~v2.0 library\footnote{cf. \lstinline{KeyGenerator::set_poly_coeffs_zero_one_negone()} at \url{https://sealcrypto.codeplex.com/SourceControl/latest#SEAL/keygenerator.h}} and~\cite{AFRICACRYPT:LepNae14}; \(\B[64]{-}\) is the default choice in HElib, cf.~\cite[Appendix C.1.1]{EPRINT:GenHalSma12} and~\cite{C:HalSho14}.
\end{remark}

It is an open question how much easier, if any, \B{}-secret LWE instances are compared to regular LWE instances. On the one hand, designers of FHE schemes typically ignore this issue~\cite{C:GenHalSma12,AFRICACRYPT:LepNae14,RSA:CosSma16}. This could be considered as somewhat justified by a reduction from~\cite{C:ACPS09} showing that an LWE instance with an arbitrary secret can be transformed into an instance with a secret following the noise distribution in polynomial time and at the loss of $n$ samples. Hence, such instances are not easier than instances with a uniformly random secret, assuming sufficiently many samples are available. As a consequence, LWE with a secret following the noise distribution is considered to be in \emph{normal form}. Given that the noise in homomorphic encryption libraries is also typically rather small --- SEAL and HElib use standard deviation $σ ≈ 3.2$ --- the distribution $ \B{-}$ gives rise to LWE instances which could be considered relatively close to normal-form LWE instances. However, considering the actual distributions, not just the standard deviations, it is known that LWE with \emph{error distribution} \(\B{}\) is insecure once sufficiently many samples are available~\cite{ICALP:AroGe11,EPRINT:ACFP14,C:KirFou15}.

On the other hand, the best, known reduction from regular LWE to \B{+}-secret LWE has an expansion factor of $\log q$ in the dimension. That is,~\cite{STOC:BLPRS13} gives a reduction from regular LWE in dimension $n$ to LWE with \(\s \sample \B{+}\) in dimension \(n \log q\).

In contrast, even for noise with width $≈\sqrt{n}$ and $\s \sample \B{-}$ the best known lattice attacks suggest an expansion factor of at most \(\log\log n\)~\cite{ACISP:BaiGal14}, if at all. Overall, known algorithms do not perform significantly better for \B{}-secret LWE instances, perhaps reinforcing our confidence in the common approach of simply ignoring the special form of the secret.

One family of algorithms has recently seen considerable progress with regards to \B{}-secret instances: combinatorial algorithms. Already in~\cite{JACM:Regev09} it was observed that the BKW algorithm, originally proposed for LPN by Blum, Kalai and Wasserman~\cite{STOC:BluKalWas00}, leads to an algorithm in $2^{Θ(n)}$ time and space for solving LWE\@. The algorithm proceeds by splitting the components of the vectors $\vec{a}_i$ into blocks of $k$ components. Then, it searches for collisions in the first block in an ``elimination table'' holding entries for (possibly) all $q^k$ different values for that block. This table is constructed by sampling fresh $(\vec{a}_i, c_i)$ pairs from the LWE oracle. By subtracting vectors with colliding components in the first block, a vector of dimension $n-k$ is recovered, applying the same subtraction to the corresponding $c_i$ values, produces an error of size $\sqrt{2}α\,q$. Repeating the process for consecutive blocks reduces the dimension further at the cost of an increase in the noise by a factor $\sqrt{2}$ at each level. This process either continues until all components of $\vec{a}_i$ are eliminated or when there are so few components left that exhaustive search can solve the remaining low-dimensional LWE instance.

A first detailed study of this algorithm when applied to LWE was provided in~\cite{DCC:ACFFP15}. Subsequently, improved variants were proposed, for small secret LWE instances via ``lazy modulus switching''~\cite{PKC:AFFP14}, via the application of an FFT in the last step of the algorithm~\cite{EC:DucTraVau15}, via varying the block size $k$~\cite{C:KirFou15} and via rephrasing the problem as the coding-theoretic problem of quantisation~\cite{C:GuoJohSta15}. In particular, the works~\cite{C:KirFou15,C:GuoJohSta15} improve the exploitation of a small secret to the point where these techniques improve the cost of solving instances where the secret is as big as the error, i.e.\ arbitrary LWE instances. Yet, combinatorial algorithms do not perform well on FHE-style LWE instances because of their large dimension $n$ to accommodate the large modulus \(q\).

\subsection{Our contribution/outline}

We first review parameter choices in HElib and SEAL as well as known algorithms for solving LWE and related problems in Section~\ref{sec:preliminaries}.

Then, we reconsider the dual-lattice attack (or ``dual attack'' in short) which finds short vectors $\vec{y}$ such that $\vec{y} ⋅ \vec{A} ≡ 0 \bmod q$ using lattice reduction. In particular, we recast this attack as the lattice-reduction analogue of the BKW algorithm and adapt techniques and lessons learned from BKW-style algorithms. Applying these techniques to parameter sets suggested for HElib and SEAL, we arrive at revised concrete and asymptotic security estimates\@.

First, in Section~\ref{sec:amortising-costs}, we recall (the first stage of) BKW as a recursive dimension reduction algorithm for LWE instances. Each step transforms an LWE instance in dimension $n$ to an instance in dimension $n-k$ at the cost of an increase in the noise by a factor of $\sqrt{2}$. This smaller instance is then reduced further by applying BKW again or solved using another algorithm for solving LWE\@; typically some form of exhaustive search once the dimension is small enough. To achieve this dimension reduction, BKW first produces elimination tables and then makes use of these tables to sample possibly many LWE samples in dimension \(n-k\) relatively cheaply. We translate this approach to lattice reduction in the low advantage regime: we perform one expensive lattice reduction step followed by many relatively cheap lattice reductions on rerandomised bases. This essentially reduces the overall solving cost by a factor of \(m\), where \(m\) is the number of samples required to distinguish a discrete Gaussian distribution with large standard deviation from uniform modulo \(q\). We note that this approach applies to any LWE instance, i.e.\ does not rely on an unusually short secret and thus gives cause for a moderate revision of many LWE estimates based on the dual-attack in the low advantage regime. It does, however, rely on the heuristic that these cheap lattice reduction steps produce sufficiently short and random vectors. We give evidence that this heuristic holds.

Second, in Section~\ref{sec:modulus-switching}, we observe that the normal form of the dual attack --- finding short vectors \(\vec{y}\) such that $\vec{y} ⋅ \vec{A} ≡ \vec{x} \bmod q$ is short --- is a natural analogue of ``lazy modulus switching''~\cite{PKC:AFFP14}. Then, to exploit the unusually small secret, we apply lattice scaling as in~\cite{ACISP:BaiGal14}. The scaling factor is somewhat analogous to picking the target modulus in modulus switching resp.~picking the (dimension of the) code for quantisation. This technique applies to any \B{}-secret LWE instance. For \(\B[h]{-}\)-secret instances, it reduces the cost of the dual attack by a factor of \(2\,L/(2\,L+1)\) in the exponent when \(\log q = \Theta\left( L \log n \right)\) for \(L\) the supported depth of FHE circuits and when \(h\) is a constant.

Third, in Section~\ref{sec:sparse-secret}, we focus on \(\s \sample \B[h]{\pm}\) and adapt the dual attack to find short vectors which produce zero when multiplied with a \emph{subset} of the columns of $\vec{A}$. This, as in BKW, produces a smaller, easier LWE instance which is then solved using another algorithm. In BKW, these smaller instances typically have very small dimension (say, $10$). Here, we consider instances with dimension of several hundreds. This is enabled by exploiting the sparsity of the secret and by relaxing the conditions on the second step: we recover a solution only with a small probability of success. The basic form of this attack does not rely on the size of the non-zero components (only on the sparsity) and reduces the cost of solving an instance in dimension \(n\) to the cost of solving an instance in dimension \(n/2\) multiplied by \(2^{h}\) where \(h\) is the hamming weight of the secret (other trade-offs between multiplicative cost increase and dimension reduction are possible and typically optimal). We also give an improved variant when the non-zero components are also small.

In Section~\ref{sec:combined}, we put everything together to arrive at our final algorithm \MYALG, which combines the techniques outlined above; inheriting their properties. We also give revised security estimates for parameter sets suggested for HElib and SEAL in Table~\ref{tab:results}. Table~\ref{tab:results} highlights that the techniques described in this work can, despite being relatively simple, produce significantly revised concrete security estimates for both SEAL and HElib.

\begin{table}
  \begin{center}
    \begin{tabular}{rrrrrr}
      \(n\) & 1024 & 2048 & 4096 & 8192 & 16384\\
      \midrule
      \multicolumn{6}{c}{SEAL~v2.0 80-bit}\\
      $\log_{2} q$     & 47.5 &   95.4 &  192.0 &  392.1  & 799.6\\
        dual  & 83.1 &   78.2 &   73.7 &   71.1 &   70.6\\
      \(\MYALG_{{\rm small}}\) & 68.1 &   69.0 &   68.2 &   68.4 &   68.8\\
      \midrule
      \multicolumn{6}{c}{HElib 80-bit}\\
      $\log_{2} q$     &  47.0 &   87.0 &  167.0 &  326.0 &  638.0\\
      dual    &  85.2 &   85.2 &   85.3 &   84.6 &   85.5\\
      \(\MYALG_{{\rm sparse}}\)  &  61.3 &   65.0 &   67.9 &   70.2 &   73.1\\
      \midrule
      \multicolumn{6}{c}{HElib 128-bit}\\
      $\log_{2} q$     &   38.0 &   70.0 &  134.0 &  261.0 &  511.0 \\
      dual    &  110.7 &  110.1 &  109.3 &  108.8 &  108.9 \\
      \(\MYALG_{{\rm sparse}}\) &   73.2 &   77.4 &   81.2 &   84.0 &   86.4 \\      
    \end{tabular}
  \end{center}
  \caption{Costs of dual attacks on HElib and SEAL~v2.0. Rows ``$\log_{2} q$'' give bit sizes for the maximal modulus for a given $n$, for SEAL it is taken from~\cite{AFRICACRYPT:LepNae14}, for HElib it is chosen such that the expected cost  is $2^{80}$ resp.\ $2^{128}$ seconds according to~\cite{C:GenHalSma12}. The rows ``dual'' give the log cost (in operations) of the dual attack according to our lattice-reduction estimates without taking any of our improvements into account; The row ``\(\MYALG_{{\rm small}}\)'' gives the log cost of Algorithm~\ref{alg:complete} with ``sparse'' set to false; The rows ``\(\MYALG_{{\rm sparse}}\)'' give the log cost of Algorithm~\ref{alg:complete} with ``sparse'' set to true. The ``sparse'' flag toggles whether the approach described in Section~\ref{sec:sparse-secret} is enabled or not in Algorithm~\ref{alg:complete}.}\label{tab:results}
\end{table}

\begin{table}
  \begin{center}
    \begin{tabular}{rrrr}
      Strategy                            & Dual  & Decode & Embed \\
      \midrule
      HElib                               & 188.9 & ---    & --- \\
      base line                           & 124.2 & 116.6  & 114.5  \\
      Section~\ref{sec:modulus-switching} & 101.0 & ---    & ---  \\
      Section~\ref{sec:sparse-secret}     & 97.1  & 111.0  & 110.9 \\
      Section~\ref{sec:combined}          & 83.9  & ---    & ---\\
    \end{tabular}    
  \end{center}
\caption{Logarithms of algorithm costs in operations mod \(q\) when applied to example parameters $n=2048$, $q ≈ 2^{63.4}$, $\alpha ≈ 2^{-60.4}$ and $\s \sample \B[64]{-}$. The row ``base line'' gives the log cost of attacks according to our lattice-reduction estimates without taking any of our improvements into account.}\label{tab:helib-2048}
\end{table}

\section{Preliminaries}\label{sec:preliminaries}

Logarithms are base 2 if not stated otherwise. We write vectors in bold, e.g.\ \a, and matrices in upper-case bold, e.g.\ $\mat{A}$. By $\a[i]$ we denote the $i$-th component of $\a$, i.e.\ a scalar. In contrast, $\a_i$ is the $i$-th element of a list of vectors. We write $\Id[m]$ for the $m × m$ identity matrix over whichever base ring is implied from context. We write \(\Ze[m × n]\) for the $m × n$ zero matrix. A lattice is a discrete subgroup of $\mathbb{R}^n$. It can be represented by a basis $\mat{B}$. We write $Λ(\mat{B})$ for the lattice generated by the rows of the matrix $\mat{B}$, i.e.\ all integer-linear combinations of the rows of $\mat{B}$. We write $Λ_q(\mat{B})$ for the $q$-ary lattice generated by the rows of the matrix $\mat{B}$ over $\ZZ_q$, i.e.\ the lattice spanned by the rows $\vec{B}$ and multiples of $q$. We write $\A[n:m]$ for the rows $n,…,m-1$ of $\A$. If the starting or end point is omitted it is assumed to be $0$ or the number of rows respectively, i.e.\ we follow Python's slice notation.

\subsection{Rolling example}\label{sec:rolling-example}

Throughout, we are going to use Example~\ref{ex:helib} below to illustrate the behaviour of the techniques described here. See Table~\ref{tab:helib-2048} for an overview of complexity estimates for solving this set of parameters using the techniques described in this work.

\begin{example}\label{ex:helib}
  The LWE dimension is $n=2048$, the modulus is $q ≈ 2^{63.4}$, the noise parameter is $\alpha ≈ 2^{-60.4}$, i.e.\ we have a standard deviation of $σ ≈ 3.2$. We have $\s \sample \B[64]{-}$, i.e.\ only $h=64$ components of the secret are $\pm 1$, all other components are zero. This set of parameters is inspired by parameter choices in  HElib and produced by calling the function \lstinline[]{fhe_params(n=2048,L=2)} of the LWE estimator from~\cite{JMC:AlbPlaSco15}.
\end{example}

\begin{comment}
  sage: print cost_str(sis(n, alpha, q, optimisation_target="lp"))
  sage: print cost_str(sis(n, alpha, q, optimisation_target="sieve"))
  sage: print cost_str(sis_small_secret_mod_switch(n, alpha, q, optimisation_target="sieve", h=64, secret_bounds=(-1,1), use_lll=True))
  sage: print cost_str(drop_and_solve(sis, n, alpha, q, optimisation_target="sieve", secret_bounds=(-1,1), h=64, postprocess=True))
  sage: print cost_str(drop_and_solve(sis_small_secret_mod_switch, n, alpha, q, optimisation_target="sieve", secret_bounds=(-1,1), h=64, postprocess=True))
\end{comment}

\subsection{Parameter choices in HElib}\label{sec:parameters-helib}

HElib~\cite{C:GenHalSma12,C:HalSho14} uses the cost of the dual attack for solving LWE to establish parameters. The dual strategy reduces the problem of distinguishing LWE from uniform to the SIS problem~\cite{STOC:Ajtai96}:

\begin{definition}[SIS]
Given \(q \in \ZZ\), a matrix \(\vec{A}\), and \(t < q\); find \(\vec{y}\) with \(0 < \| \vec{y} \| \leq t\) and \[\vec{y} ⋅ \vec{A} ≡ \vec{0} \pmod{q}.\]
\end{definition}

Now, given samples \(\vec{A}, \vec{c}\) where either \(\vec{c} = \vec{A}⋅\vec{s} + \vec{e}\) or \(\vec{c}\) uniform, we can distinguish the two cases by finding a short \(\vec{y}\) which solves SIS on \(\vec{A}\) and by computing \(\ip{\vec{y}}{\vec{c}}\). On the one hand, if \(\vec{c} = \vec{A}⋅\vec{s} + \vec{e}\), then \(\ip{\vec{y}}{\vec{c}} = \ip{\vec{y}⋅\vec{A}}{\vec{s}} + \ip{\vec{y}}{\vec{e}} \equiv \ip{\vec{y}}{\vec{e}} \pmod{q}\). If \(\vec{y}\) is short then \(\ip{\vec{y}}{\vec{e}}\) is also short. On the other hand, if \(\vec{c}\) is uniformly random, so is \(\ip{\vec{y}}{\vec{c}}\).

To pick a target norm for \(\vec{y}\), HElib picks $\vecnorm{\vec{y}} = q$ which allows distinguishing with good probability because \(q\) is not too far from \(q/\sigma\) since \(\sigma \approx 3.2\) and \(q\) is typically rather large. More precisely, we may rely on the following lemma:

\begin{lemma}[\cite{RSA:LinPei11}]\label{lem:distinguishing-advantage}
Given an LWE instance characterised by \(n\), \(α\), \(q\) and a vector \(\vec{y}\) of length \(\|\vec{y}\|\) such that \(\vec{y} ⋅ \vec{A} \equiv 0 \pmod{q}\), the advantage of distinguishing \(\ip{\vec{y}}{\vec{e}}\) from random is close to  \[\exp(-π{(\|\vec{y}\| ⋅ α)}^2).\]
\end{lemma}

To produce a short enough $\vec{y}$, we may call a lattice-reduction algorithm. In particular, we may call the BKZ algorithm with block size $β$. After performing BKZ-$β$ reduction the first vector in the transformed lattice basis will have norm $δ_0^m ⋅ {\det(Λ)}^{1/m}$ where $\det(Λ)$ is the determinant of the lattice under consideration, $m$ its dimension and the root-Hermite factor $δ_0$ is a constant based on the block size parameter $β$. Increasing the parameter $β$ leads to a smaller $δ_0$ but also leads to an increase in run-time; the run-time grows at least exponential in $β$ (see below).

In our case, the expression above simplifies to $\vecnorm{\vec{y}} ≈ δ_0^m ⋅ q^{n/m}$ whp, where $n$ is the LWE dimension and $m$ is the number of samples we consider. The minimum of this expression is attained at $m = \sqrt{\frac{n\,\log q}{\log δ_0}}$~\cite{PQCBook:MicReg09}.

Explicitly, we are given a matrix $\A \in \ZZq^{m × n}$, construct a basis $\mat{Y}$ for its left kernel modulo $q$ and then consider the $q$-ary lattice $Λ_q(\mat{Y})$ spanned by the rows of $\mat{Y}$. With high probability $\mat{Y}$ is an $(m-n) × m$ matrix and $Λ_q(\mat{Y})$ has volume $q^n$. Let $\mat{L}$ be a basis for $Λ_q(\mat{Y})$, $m' = m-n$ and write $\mat{Y} = [\Id[m']|\mat{Y}']$ then we have
\begin{align*}
  \mat{L} = \begin{pmatrix}
    \Id[m'] & \mat{Y}'\\
    0 & q\,\Id[n]
  \end{pmatrix}.
\end{align*}
In other words, we are attempting to find a short vector $\vec{y}$ in the integer row span of $\mat{L}$.

Given a target for the norm of $\mat{y}$ and hence for $δ_0$, HElib\footnote{\url{https://github.com/shaih/HElib/blob/a5921a08e8b418f154be54f4e39a849e74489319/src/FHEContext.cpp#L22}} estimates the cost of lattice reduction by relying on the following formula from~\cite{RSA:LinPei11}:
\begin{equation}
  \log{t_{BKZ}(δ_0)} = \frac{1.8}{\log{δ_0}}-110, \label{eq:lindner-peikert}
\end{equation}
where \(t_{BKZ}(δ_0)\) is the time in seconds it takes to BKZ reduce a basis to achieve root-Hermite factor \(δ_{0}\). This estimate is based on experiments with BKZ in the NTL library~\cite{NTL} and extrapolation.

\subsection{LP model}\label{sec:lindner-peikert}

The~\cite{RSA:LinPei11} model for estimating the cost of lattice-reduction is not correct.

Firstly, it expresses runtime in seconds instead of units of computation. As Moore's law progresses and more parallelism is introduced, the number of instructions that can be performed in a second increases. Hence, we first must translate Eq.~\eqref{eq:lindner-peikert} to units of computation. The experiments of Lindner and Peikert were performed on a 2.33~Ghz AMD~Opteron machine, so we may assume that about $2.33 ⋅ 10^9$ operations can be performed on such a machine in one second and we scale Eq.~\eqref{eq:lindner-peikert} accordingly.\footnote{The number of operations on integers of size $\log q$ depends on \(q\) and is not constant. However, constant scaling provides a reasonable approximation for the number of operations for the parameter ranges we are interested in here.}

Secondly, the LP model does not fit the implementation of BKZ in NTL\@. The BKZ algorithm internally calls an oracle for solving the shortest vector problem in smaller dimension. The most practically relevant algorithms for realising this oracle are enumeration without preprocessing (Fincke-Pohst) which costs $2^{Θ(β^2)}$ operations, enumeration with recursive preprocessing (Kannan) which costs $\beta^{Θ(β)}$ and sieving which costs $2^{Θ(β)}$. NTL implements enumeration without preprocessing. That is, while it was shown in~\cite{ICITS:Walter15} that BKZ with recursive BKZ pre-processing achieves a run-time of $\poly ⋅ \beta^{Θ(β)}$, NTL does not implement the necessary recursive preprocessing with BKZ in smaller dimensions.  Hence, it runs in time \(\poly ⋅ 2^{Θ(β^2)}\) for block size \(β\).

Thirdly, the LP model assumes a linear relation between \(1/\log(δ_0)\) and the log of the running time of BKZ, but from the ``lattice rule-of-thumb'' (\(δ_0 ≈ β^{1/(2β)}\)) and $2^{Θ(β)}$ being the complexity of the best known algorithm for solving the shortest vector problem, we get:

\begin{lemma}[\cite{JMC:AlbPlaSco15}]\label{lem:complexity-delta0}
The log of the time complexity achieve a root-Hermite factor \(δ_0\) with BKZ is
\[Θ \left( \frac{\log(1/\log δ_0)}{\log δ_0} \right)\]
if calling the SVP oracle costs \(2^{Θ(β)}\).
\end{lemma}

To illustrate the difference between Lemma~\ref{lem:complexity-delta0} and Eq.~\eqref{eq:lindner-peikert}, consider Regev's original parameters~\cite{STOC:Regev05} for LWE\@: $q ≈ n^2$, $α\,q ≈ \sqrt{n}$. Then, solving LWE with the dual attack and advantage $\epsilon$ requires a log root-Hermite factor $\log δ_0 ={\log^2{\left(\alpha {\sqrt{\ln({1/ε})/π}}^{-1} \right)}}/{(4n \log{q})}$~\cite{JMC:AlbPlaSco15}. Picking $ε$ such that \(\log{\sqrt{\ln(1/ε)/π}} ≈ 1\), the log root-Hermite factor becomes \(\log δ_0 = \frac{9\, \log n }{32\,n}\). Plugging this result into Eq.~\ref{eq:lindner-peikert}, we would estimate that solving LWE for these parameters takes \( \log t_{BKZ}(δ_0) = \frac{32\, n}{5\, \log n }-110\) seconds, which is subexponential in $n$.

\subsection{Parameter choices in SEAL v2.0}\label{sec:parameters-seal}

SEAL~v2.0~\cite{LaiPla16} largely leaves parameter choices to the user. However, it provides the \lstinline{ChooserEvaluator::default_parameter_options()} function which returns values from~\cite[Table~2]{AFRICACRYPT:LepNae14}.\footnote{Note that the most recent version of SEAL now recommends more conservative parameters~\cite{LaiChePla16}, partly in reaction to this work.} This table gives a maximum $\log q$ for 80 bits of security for $n=1024, 2048, 4096, 8192, 16384$. We reproduce these values for \(\log q\) in Table~\ref{tab:results}. The default standard deviation is $σ=3.19$.

The values of~\cite[Table~2]{AFRICACRYPT:LepNae14} are based on enumeration costs and the simulator from~\cite{AC:CheNgu11,CheNgu12}. Furthermore, to extrapolate from available enumeration costs from~\cite{CheNgu12},~\cite{AFRICACRYPT:LepNae14} assumes calling the SVP oracle in BKZ grows only exponentially with \(\beta\), i.e. as \(2^{0.64\beta - 28}\). Note that this is overly optimistic, as~\cite{CheNgu12} calls enumeration with recursive preprocessing to realise the SVP oracle inside BKZ, which has a complexity of $\beta^{\Theta(β)}$. 

Finally, we note that the SEAL~v2.0 manual~\cite{LaiPla16} cautions the user against relying on the security provided by the list of default parameters.

\subsection{Lattice reduction}\label{sec:lattice-reduction}

We will estimate the cost of lattice reduction using the following assumptions: BKZ-$β$ produces vectors with $δ_0 ≈ {\left( \frac{β}{2 π e} {(π β)}^{\frac{1}{β}}  \right)}^{\frac{1}{2(β-1)}}$~\cite{PhD:Chen13}. The SVP oracle in BKZ is realised using sieving and sieving in blocksize $β$ costs $t_β = 2^{0.292\,β + 12.31}$ clock cycles. Here, $0.292\,β$ follows from~\cite{SODA:BDGL16}, the additive constant $+ 12.31$ is based on experiments in~\cite{C:Laarhoven15}. BKZ-$β$ costs $c\,n \cdot t_β$ clock cycles in dimension $n$ for some small constant \(c\) based on experiments in~\cite{PhD:Chen13}; cf.~\cite[Figure~4.6]{PhD:Chen13}. This corresponds roughly to \(2\,c\) tours of BKZ\@. We pick \(c=8\) based on our experiments with~\cite{fplll}.

This estimate is more optimistic than the estimate in~\cite{JMC:AlbPlaSco15}, which does not yet take~\cite{SODA:BDGL16} into account and bases the number of SVP oracle calls on theoretical convergence results~\cite{C:HanPujSte11} instead of experimental evidence. On the other hand, this estimate is more pessimistic than~\cite{CCS:BCDMNN16} which assumes \emph{one} SVP call to be sufficient in order to protect against future algorithmic developments. While such developments, amortising costs across SVP calls during one BKZ reduction, are plausible, we avoid this assumption here in order not to ``oversell'' our results. However, we note that our improvements are somewhat oblivious to the underlying lattice-reduction model used. That is, while the concrete estimates for bit-security will vary depending on which estimate is employed, the techniques described here lead to improvements over the plain dual attack regardless of model. For completeness, we give estimated costs in different cost models in Appendix~\ref{sec:alternative-cost-models}.

According to the~\cite{RSA:LinPei11} estimate, solving Example~\ref{ex:helib} costs $2^{157.8}$ seconds or $2^{188.9}$ operations using the standard dual attack. The estimates outlined in this section predict a cost of $2^{124.2}$ operations for the same standard dual attack.

\begin{comment}
attach("estimator/estimator.py")
n, alpha, q = fhe_params(2, 2048)
print cost_str(sis(n, alpha, q, optimisation_target="lp"))
188.9 - log(2.33*10^9,2.0)
\end{comment}

\subsection{Related work}\label{sec:other-algorithms}

\subsubsection{LWE.}

Besides the dual attack, via BKW or lattice-reduction, there is also the primal attack, which solves the bounded distance decoding (BDD) problem directly. That is, given $\left(\vec{A},\vec{c}\right)$ with $\vec{c} = \vec{A} ⋅ \vec{s} + \vec{e}$ or $\vec{c} \sample \U{\ZZq^m}$ find  \(\vec{s'}\) such that \(\abs{\vec{w} - \vec{c}}\)  with  \(\vec{w} = \vec{A} ⋅ \vec{s'}\)  is minimised. For this, we may employ Kannan's embedding~\cite{ICISC:AlbFitGop13} or variants of Babai's nearest planes after lattice reduction~\cite{RSA:LinPei11,RSA:LiuNgu13}. For Example~\ref{ex:helib} the cost of the latter approach is $2^{116.6}$ operations, i.e.~about a factor 190 faster than the dual attack.

Arora \& Ge proposed an asymptotically efficient algorithm for solving LWE~\cite{ICALP:AroGe11}, which was later improved in~\cite{EPRINT:ACFP14}. However, these algorithms involve large constants in the exponent, ruling them out for parameters typically considered in cryptography. We, hence, do not consider them further in this work.

\subsubsection{Small-secret LWE.} As mentioned in~\cite{EPRINT:GenHalSma12}, we can transform instances with an unusually short secret into instances where the secret follows the error distribution, but $n$ samples have the old, short secret as noise~\cite{C:ACPS09}.

Given a random $m × n$ matrix $\mat{A} \bmod q$ and an $m$-vector \(\vec{c} = \vec{A} ⋅ \vec{s} + \vec{e} \bmod q\), let $\vec{A}_0$ denotes the first $n$ rows of $\vec{A}$, $\vec{A}_1$ the next $n$ rows, etc., $\vec{e}_0, \vec{e}_1, …$  are the corresponding parts of the error vector and $\vec{c}_0 , \vec{c}_1, …$ the corresponding parts of $\vec{c}$. We have $\vec{c}_0 = \vec{A}_0 \cdot \vec{s}  + \vec{e}_0$ or $\vec{A}_0^{-1} \cdot \vec{c}_0 = \vec{s} + \vec{A}_0^{-1} \vec{e}_0$. For $i > 0$ we have $\vec{c}_i = \vec{A}_i \cdot \vec{s} + \vec{e}_i$, which together with the above gives \(\vec{A}_i \vec{A}_0^{-1} \vec{c}_0 - \vec{c}_i = \vec{A}_i \vec{A}_0^{-1} \vec{e}_0 - \vec{e}_i\). The output of the transformation is  \(\vec{z} = \vec{B} ⋅ \vec{e}_0  + \vec{f}\) with $\vec{B}  = (\vec{A}_0^{-1}\mid \vec{A}_1 ⋅ \vec{A}_0^{-1}\mid \dots)$ and $\vec{z} = (\vec{A}_0^{-1} \vec{c}_0 \mid \vec{A}_1 \vec{A}_0^{-1} \vec{c}_1 \mid \ldots)$ and $\vec{f} = (\vec{s}|\vec{e}_1 \mid \dots)$. For Example~\ref{ex:helib}, this reduces $α$ from $2^{-60.4}$ to $≈2^{-60.8}$ and marginally improves the cost of solving.

An explicit variant of this approach is given in~\cite{ACISP:BaiGal14}. Consider the lattice \[Λ=\{\vec{v} \in \ZZ^{n+m+1} \mid \left[\vec{A} \mid \vec{I}_m \mid -\vec{c} \right] ⋅ \vec{v} ≡ 0 \bmod{q} \}.\]
It has an unusually short vector \((\vec{s} || \vec{e} || 1)\). When \(\|\vec{s}\| \ll \|\vec{e}\|\), the vector \((\vec{s} || \vec{e} || 1)\) is uneven in length. To balance the two sides, rescale the first part to have the same norm as the second. When \(\vec{s} \sample \B{-}\), this scales the volume of the lattice by \(\sigma^n\). When \(\vec{s} \sample \B{+}\), this scales the volume of the lattice by \({(2\sigma)}^n\) because we can scale by \(2\sigma\) and then re-balance. When \(\vec{s} \sample \B[h]{±}\), the volume is scaled depending on \(h\). For our rolling example, this approach costs $2^{114.5}$ operations, i.e.\ is about a factor 830 faster than the dual attack.

Independently and concurrently to this work, a new key-exchange protocol based on sparse secret LWE was proposed in~\cite{ICISC:CKHSL16}. A subset of the techniques discussed here are also discussed in~\cite{ICISC:CKHSL16}, in particular, ignoring components of the secret and using lattice scaling as in~\cite{ACISP:BaiGal14}.

\subsubsection{Combinatorial.}

This work combines combinatorial and lattice-reduction techniques. As such, it has some similarities with the hybrid attack on NTRU~\cite{C:HowgraveGraham07}. This attack was recently adapted to LWE in the \B{}-secret case in~\cite{AFRICACRYPT:BGPW16} and its complexity revisited in~\cite{EPRINT:Wunderer16}.

\subsubsection{Rings.}

Recently,~\cite{C:AlbBaiDuc16} proposed a subfield lattice-attack on the two fully homomorphic encryption schemes YASHE~\cite{IMA:BLLN13} and LTV~\cite{STOC:LopTroVai12}, showing that NTRU with ``overstretched'' moduli $q$ is less secure than initially expected. Quickly after,~\cite{EPRINT:KirFou16} pointed out that the presence of subfields is not necessary for attacks to succeed. NTRU can be considered as the homogeneous version of Ring-LWE, but there is currently no indication that these attacks can be translated to the Ring-LWE setting. There is currently no known algorithm which solves Ring-LWE faster than LWE for the parameter choices (ring, error distribution, etc.) typically considered in FHE schemes.

\section{Amortising costs}\label{sec:amortising-costs}

If the cost of distinguishing LWE from random with probability $ε$ is $c$, the cost of solving is customary estimated as at least $c/ε$~\cite{RSA:LinPei11}. More precisely, applying Chernoff bounds, we require about $1/ε^2$ samples to amplify a decision experiment succeeding with advantage $ε$ to a constant advantage. Hence, e.g.\  in~\cite{JMC:AlbPlaSco15}, the dual attack is costed as the cost of running BKZ-$β$ to achieve the target $δ_0$ multiplied by the number of samples required to distinguish with the target advantage, i.e.\ \(≈ c/ε^{2}\).

In the case of the dual attack, this cost can be reduced by performing rerandomisation on the already reduced basis. If $\mat{L}$ is a basis for the lattice $Λ_q(\mat{Y})$, we first compute $\mat{L}'$ as the output of BKZ-$β$ reduction where $β$ is chosen to achieve the target $δ_0$ required for some given target advantage. Then, in order to produce sufficiently many relatively short vectors $\vec{y}_i \in Λ_q(\mat{Y})$ we repeatedly multiply $\mat{L}'$ by a fresh random sparse unimodular matrix with small entries to produce $\mat{L}_i'$. As a consequence, $\mat{L}_i'$ remains somewhat short. Finally, we run BKZ-$β'$ with $β' ≤ β$ on $\mat{L}_i'$ and return the smallest non-zero vector as $\vec{y}_i$. See Algorithm~\ref{alg:amortising}, where \(\varepsilon_{d}\) is chosen following Lemma~\ref{lem:distinguishing-advantage} (see below for the expectation of $\vecnorm{\vec{y}}$) and \(m\) is chosen following~\cite{SarLun12}.

That is, similar to BKW, which in a first step produces elimination tables which allow sampling smaller dimensional LWE samples in $\bigO{n^2}$ operations, we first produce a relatively good basis $\mat{L}'$ to allow sampling $\vec{y}_i$ relatively efficiently.

To produce the estimates in  Table~\ref{tab:results}, we assume the same rerandomisation strategy as is employed in fplll's implementation~\cite{fplll} of extreme pruning for BKZ~2.0.\footnote{\url{https://github.com/fplll/fplll/blob/b75fe83/fplll/bkz.cpp#L43}} This rerandomisation strategy first permutes rows and then adds three existing rows together using $\pm 1$ coefficients, which would increase norms by a factor of $\sqrt{3} < 2$ when all vectors initially have roughly the same norm. For completeness, we reproduce the algorithm in Appendix~\ref{sec:rerandomisation}. We then run LLL, i.e.~we set $β'=2$, and  assume that our $\vec{y}_i$ have their norms increased by a factor of two, i.e.\ $\E{\vecnorm{\vec{y}_i}} = 2 ⋅ δ_0^m q^{n/m}$.

\begin{algorithm}
  \KwData{candidate LWE samples \(\mat{A},\vec{c} \in \ZZq^{m × n}  × \ZZq^{m}\)}
  \KwData{BKZ block sizes \(β, β' ≥ 2\)}
  \KwData{target success probability \(ε\)}
  \(ε_{d} \gets \exp(-π{(\E{\vecnorm{\vec{y}_{i}}}⋅α)}^{2})\)\; 
  \(m \gets \lceil2\,\log(2 - 2\,ε)/\log(1 - 4\, ε_{d}^{2})\rceil\)\;       
  \(\mat{L} \gets\) basis for \(\{\vec{y} \in \ZZ^m : \vec{y} ⋅ \mat{A} ≡ 0 \bmod q\}\)\;
  \(\mat{L}' \gets\) BKZ-\(β\) reduced basis for \(\mat{L}\)\;
  \For{\(i \gets 0\) \KwTo{} \(m-1\)}{
    \(\mat{U} \sample \) a sparse unimodular matrix with small entries\;
    \(\mat{L}_{i} \gets \) \(\mat{U} ⋅ \mat{L}'\)\;
    \(\mat{L}'_{i} \gets \) BKZ-\(β'\) reduced basis for \(\mat{L}_{i}\)\;
    \(\vec{y}_{i} \gets\) shortest row vector in \(\mat{L}'_{i}\)\;
    \(e'_{i} \gets \ip{\vec{y}_{i}}{\vec{c}}\)\;      
  }
  \eIf{\(e'_{i}\) follow discrete Gaussian distribution}{
    \Return\(\top\)\;
  }{
    \Return\(\bot\)\;
  }
 \caption{\small \MYALG[1]: Amortising costs in BKW-style SIS strategy for solving LWE }\label{alg:amortising}
\end{algorithm}

\subsubsection{Heuristic.} We note that, in implementing this strategy, we are losing statistical independence. To maintain statistical independence, we would consider fresh LWE samples and distinguish $\ip{\vec{y}_i}{\e_i}$ from uniform. However, neither HElib nor SEAL provides the attacker with sufficiently many samples to run the algorithm under these conditions. Instead, we are attempting to distinguish $\ip{\vec{y}_i}{\e}$ from uniform. Furthermore, since we are performing only light rerandomisation our distribution could be skewed if our \(\vec{y}_{i}\) in $\ip{\vec{y}_i}{\e}$ are not sufficiently random. Just as in BKW-style algorithms~\cite{DCC:ACFFP15} we assume the values $\ip{\vec{y}_i}{\e}$ are distributed closely enough to the target distribution to allow us to ignore this issue.

\subsubsection{Experimental verification.} We tested the heuristic assumption of Algorithm~\ref{alg:amortising} by rerandomising a BKZ-60 reduced basis using Algorithm~\ref{alg:rerandomisation} with \(d=3\) followed by LLL reduction several hundred times. In this experiment, we recovered fresh somewhat short vectors in each call, where somewhat short means with a norm at most twice that of the shortest vector of \(\mat{L}'\). We give further experimental evidence in Section~\ref{sec:combined}.

Finally, we note that this process shares some similarities with random sampling reduction (RSR)~\cite{STACS:Schnorr03}, where random linear combinations are LLL reduced to produce short vectors. While, here, we are only performing sparse sums and accept \emph{larger} norms, the techniques used to analyse RSR might permit reducing our heuristic to a more standard heuristic assumption.

\section{Scaled normal-form}\label{sec:modulus-switching}

The line of research improving the BKW algorithm for small secrets starting with~\cite{PKC:AFFP14} proceeds from  the observation that we do not need to find \(\vec{y} ⋅ \vec{A} ≡ 0 \bmod q\), but if the secret is sufficiently small then any $\vec{y}$ such that \(\vec{y} ⋅ \vec{A}\) is short suffices, i.e.\ we seek short vectors $(\vec{w},\vec{v})$ in the lattice \[Λ = \{(\vec{y},\vec{x}) \in \ZZ^{m} × \ZZ^n : \vec{y}⋅ \vec{A} ≡ \vec{x} \bmod q\}.\]

Note that this lattice is the lattice considered in dual attacks on normal form LWE instances (cf.~\cite{EPRINT:ADPS15}).\footnote{The strategy seems folklore, we were unable to find a canonical reference for it.} Given a short vector in \((\vec{w},\vec{v}) \in Λ\), we have
\[\vec{w}⋅\vec{c} = \vec{w}⋅(\vec{A}⋅\vec{s} + \vec{e}) = \ip{\vec{v}}{\vec{s}} + \ip{\vec{w}}{\vec{e}}.\] Here, $\vec{v}$ corresponds to the noise from ``modulus switching'' or quantisation in BKW-style algorithms and $\vec{w}$ to the multiplicative factor by which the LWE noise increases due to repeated subtractions.

Now, in small secret LWE instances we have $\|{\s}\| < \|{\vec{e}}\|$. As a consequence, we may permit \(\|\v\| > \|\w\|\) such that \[\|\ip{\vec{w}}{\vec{s}}\| ≈ \|\ip{\vec{v}}{\vec{e}}\|.\] Hence, we consider the lattice
\[Λ_c = \{(\vec{y}, \vec{x}/c) \in \ZZ^{m} × {({1}/{c} ⋅ \ZZ)}^n : \vec{y} ⋅ \vec{A} ≡ \vec{x} \bmod q\} \] for some constant \(c\), similar to~\cite{ACISP:BaiGal14}. The lattice $Λ_c$ has dimension $m'=m+n$ and whp volume ${(q/c)}^n$. To construct a basis for $Λ_c$, assume $\A[m-n:m]$ has full rank (this holds with high probability for large \(q\)). Then $Λ_c = Λ(\mat{L'})$ with 
\begin{align*}
  \mat{L}' = \begin{pmatrix}
    \frac{1}{c}\Id[n] & \Ze[n × (m-n)] & \A_{m-n:m}^{-1}\\
                      & \Id[m-n]       & \mat{B}'\\
                      &                & q\Id[n]\\
  \end{pmatrix}
\end{align*}
where $[\Id[m-n] | \mat{B}']$ is a basis for the left kernel of $\A \bmod q$.

\begin{remark}
  In our estimates for HElib and SEAL, we typically have \(m=n\) and $[\Id[m-n] | \mat{B}'] \in \ZZ^{0 \times n}$. 
\end{remark}

It remains to establish $c$. Lattice reduction produces a vector \((\vec{w},\vec{v})\) with
\begin{equation}\label{eq:modulus-switching-norm}
\|(\vec{w},\vec{v})\| ≈ δ_0^{m'}⋅ {(q/c)}^{n/m'},
\end{equation}
 which translates to a noise value
\[e = \vec{w} ⋅ \vec{A} ⋅ \vec{s} + \ip{\vec{w}}{\vec{e}}  = \ip{c⋅ \vec{v}}{\vec{s}} + \ip{\vec{w}}{\vec{e}}\]
and we set \[c = \frac{α\,q}{\sqrt{2\,\pi\, h}} ⋅ \sqrt{m' - n}\] to equalise the noise contributions of both parts of the above sum.

As a consequence, we arrive at the following lemma, which is attained by combining Equation~\eqref{eq:modulus-switching-norm} with Lemma~\ref{lem:distinguishing-advantage}.

\begin{lemma}\label{lem:modulus-switching}
Let {\(m'=2\,n\)} and \(c = \frac{α\,q}{\sqrt{2\,\pi\,h}} ⋅ \sqrt{m' - n}\). A lattice reduction algorithm achieving \(δ_0\) such that 
\[\log δ_0 =
  \frac{\log\left(\frac{2 \, n \log^{2} ε}{π α^{2} h}\right)}{8 \, n}\]
leads to an algorithm solving decisional LWE with \(\s \sample \B[h]{-}\) instance with advantage \(ε\) and the same cost.
\end{lemma}

\begin{remark}
We focus on $m' = 2\,n$ in Lemma~\ref{lem:modulus-switching} for ease of exposure. For the instances considered in this work, $m' = 2\,n$ is a good approximation for $m'$ (see Section~\ref{sec:combined}).
\end{remark}

For Example~\ref{ex:helib} we predict at a cost of \(2^{107.4}\) operations mod \(q\) for solving Decision-LWE when applying this strategy. Amortising costs as suggested in Section~\ref{sec:amortising-costs} reduces it further to \(2^{{101.0}}\) operations mod \(q\).

\subsubsection{Asymptotic behaviour.} The general dual strategy, without exploiting small secrets, requires\[\log δ_0 = \frac{\log\left(-\frac{2 \, \log ε}{\alpha^{2} q}\right)}{4 \, n}\] according to~\cite{JMC:AlbPlaSco15}. For HElib's choice of $8 = α\,q$ and $h=64$ and setting \(ε\) constant, this expression simplifies to \[\log δ_0 = \frac{\log q  + C_{d}}{4\,n},\] for some constant \(C_{d}\). On the other hand, Lemma~\ref{lem:modulus-switching} simplifies to
\begin{equation}
  \label{eq:complexity-delta0}
  \log δ_0 = \frac{\log q  + \frac{1}{2}\log n + C_{m}}{4\,n},  
\end{equation}
for some constant \(C_{m} < C_{d}\).

For a circuit of depth \(L\), BGV requires \(\log q =  L \log n + \bigO{L}\) \cite[Appendix~C.2]{EPRINT:GenHalSma12}. Applying Lemma~\ref{lem:complexity-delta0}, we get that \[\lim_{\kappa \rightarrow \infty} \frac{\textnormal{cost}_{m}}{\textnormal{cost}_{d}} = \lim_{n \rightarrow \infty} \frac{\textnormal{cost}_{m}}{\textnormal{cost}_{d}} = \frac{2\,L}{2\,L +1} ,\] where \(\textnormal{cost}_{d}\) is the log cost of the standard dual attack, \(\textnormal{cost}_{m}\) is the log cost under Lemma~\ref{lem:modulus-switching} and \(\kappa\) the security parameter. The same analysis applies to any constant \(h\). Finally, when \(h=2/3\,n\), i.e.~\(\s \sample \B{-}\), then the term \(1/2\cdot \log n\) vanishes from~\eqref{eq:complexity-delta0}, but \(C_{m}>C_{d}\).

\begin{comment}
  sage: f1 = symbolic_modulus_switching(n, 8/q, q, h=64, m=2*n, epsilon=1/2)
  sage: f1.canonicalize_radical()
  sage: f2 = symbolic_sis(n, 8/q, q, m=2*n, epsilon=1/2)
  sage: f2.canonicalize_radical()
\end{comment}

\section{Sparse secrets}\label{sec:sparse-secret}

Recall that BKW-style algorithms consist of two stages or, indeed, sub-algorithms. First, in the reduction stage, combinatorial methods are employed to transform an LWE instance in dimension $n$ into an instance of dimension $0 ≤ n' ≤ n$, typically with increased noise level $α$. This smaller LWE instance is then, in the solving stage, is solved using some form of exhaustive search over the secret.

Taking the same perspective on the dual attack, write $\mat{A} = [\mat{A}_0 \mid \mat{A}_1]$ with $\mat{A}_0 \in \ZZ_q^{m × (n-k)}$ and $\mat{A}_1 \in \ZZ_q^{m × k}$ and find a short vector in the lattice
\[  Λ = \{\vec{y} \in \ZZ^m : \vec{y} ⋅ \mat{A}_0 ≡ 0 \bmod q\}.\]

Each short vector $\vec{y} \in Λ$ produces a sample for an LWE instance in dimension $k$ and noise rate $α' = \E{\vecnorm{\vec{y}}} ⋅ α$. Setting $k=0$ recovers the original dual attack. For $k>0$, we may now apply our favourite algorithm for solving small dimensional, easy LWE instances. Applying exhaustive search implies  $\log_2{k} < κ$ for $\s \sample \B{+}$ resp.\ $\log_3{k} <κ$ for $\s \sample \B{-}$ when $κ$ is the target level of security.

The case $\s \sample \B[h]{\pm}$ permits much larger $k$ by relaxing the conditions we place on solving the $k$-dimensional instance. Instead of solving with probability one, we solve with some probability $p_k$ and rerun the algorithm in case of failure.

For this, write $\mat{A} ⋅ \mat{P} = [\mat{A}_0 \mid \mat{A}_1]$ and $\s ⋅ \mat{P} = [\s_0 \mid \s_1]$ where $\mat{P}$ is a random permutation matrix. Now, over the choice of $\mat{P}$ there is a good chance that $\s_1 = 0$ and hence that $\mat{A}_1 ⋅ \s_1 ≡ 0 \bmod q$. That is, the right choice of $\mat{P}$ places all non-zero components of $\s$ in the $\s_0$ part.

In particular, with probability $1-h/n$ a coordinate $\s[i]$ is zero. More generally, picking $k$ components of $\s$ at random will pick only components such that $\s[i] = 0$ with probability

\[p_{k} = \prod_{i=0}^{k-1} \left(   1- \frac{h} {n-i} \right) = \frac{\binom{n-h}{k}}{\binom{n}{k}} ≈ {\left(1-\frac{h}{n}\right)}^k.\]

Hence, simply treating $k>0$ in the solving stage the same as $k=0$ succeeds with probability $p_{k}$. The success probability can be amplified to close to one by repeating the elimination and solving stages  $≈1/p_{k}$ times assuming we distinguish with probability close to 1.

It is clear that the same strategy translates to the primal attack by simply dropping random columns before running the algorithm. However, for the dual attack, the following improvement can be applied. Instead of considering only $\s_1 = 0$, perform exhaustive search over those solutions that occur with sufficiently high probability. In particular, over the choice of $\mat{P}$, the probability that $\s_1$ contains \(k-j\) components with \(\s_{1,(i)}=0\) and exactly \(j\) components with \(\s_{1,(i)} ≠ 0\) is
\[p_{k,j} = \frac {{\binom{n-h}{k-j}}{\binom{h}{j}}}{\binom{n}{k}},\]
i.e.~follows the hypergeometric distribution.

Now, assuming \(\s \sample \B[h]{-}\), to check if any of those candidates for $\s_1$ is correct, we need to compare $\binom{k}{j} ⋅ 2^j$ distributions against the uniform distribution mod $q$.

Thus, after picking a parameter $ℓ$ we arrive at Algorithm~\ref{alg:sparse} with cost:

\begin{enumerate}
\item \(m\) calls to BKZ-$β$ in dimension $n-k$.
\item \(m ⋅ \sum_{i=0}^{ℓ} \binom{k}{i} ⋅ 2^i ⋅ i \) additions mod $q$ to evaluate $m$ samples on all possible solutions up to weight $ℓ$.
\end{enumerate}

Assuming \(m\) is chosen such that distinguishing LWE from uniform succeeds with probability close to one, then Algorithm~\ref{alg:sparse} succeeds with probability  \({\sum_{j=0}^{ℓ} p_{k,j}}\).

\begin{algorithm}
  \KwData{\(m \times n\) matrix \(\mat{A}\) over \(\ZZq\)}
  \KwData{\(m\) vector \(\vec{c}\) over \(\ZZq\)}
  \KwData{density parameter \(0 ≤ \ell≤ 64\)}
  \KwData{dimension parameter \(0 \leq k \leq n\)}
  \(\mat{P} \sample\) \(n \times n\) permutation matrices\;
  \([\mat{A}_{0} \mid \mat{A}_{1}] \gets \mat{A} ⋅ \mat{P}\) with \(\mat{A}_{0} \in \ZZ_{q}^{m \times (n-k)}\)\;
  \(\mat{L} \gets\) basis for scaled-dual lattice of \(\mat{A}_{0}\)\;
  \For{\(i \gets 0\) \KwTo{} \(m-1\)}{
    \(\vec{y}_{i} \gets\) a short vector in the row span of \(\mat{L}\)\;
    \(e'_{i} \gets \ip{\vec{y}_{i}}{\vec{c}}\)\;
  }
  \If{\(e'_{i}\) follow discrete Gaussian distribution}{
    \Return\(\top\)\;}
  \ForEach{\(\s'\) in the set of \(\sum_{i=0}^{ℓ} \binom{k}{i} ⋅ 2^i\) candidate solutions}{
    \For{\(i \gets 0\) \KwTo{} \(m-1\)}{
      \(e''_{i} = e'_{i} + \ip{\vec{y}_{i} \cdot \mat{A}_{1}}{\s'}\)\;
    }
    \If{\(e''_{i}\) follow discrete Gaussian distribution}{\Return\(\top\)\;}      
  }
  \Return{\(\bot\)}\;
  \caption{\small \MYALG[2]: Sparse secrets in BKW-style SIS strategy for solving LWE.}\label{alg:sparse}
\end{algorithm}

\subsubsection{Asymptotic behaviour.} We arrive at the following simple lemma:

\begin{lemma}

Let $0≤ h<n$ and \(d > 1\) be constants, \(p_{h,d}\) be some constant depending on \(h\) and \(d\), $c_{n,α,q}$ be the cost of solving LWE with parameters $n, α, q$ with probability \(\geq 1 - 2^{-p_{h,d}^{2}}\)  Then, solving LWE in dimension $n$ with $\s \sample \B[h]{±}$ costs $\bigO{c_{n-n/d, α, q}}$ operations.
\end{lemma}

\begin{proof}
Observe that $p_{h,d} = \lim_{n → ∞} {\binom{n-h}{n/d}}/{\binom{n}{n/d}}$ is a constant for any constant $0 ≤ h < n$ and $d > 1$. Hence, solving $\bigO{1/p_{h,d}} = \bigO{1}$ instances in dimension $n-n/d$ solves the instance in dimension $n$. \qed{}
\end{proof}

\begin{remark}
  Picking $d=2$ we get $\lim_{n → ∞} {\binom{n-h}{n/2}}/{\binom{n}{n/2}} = 2^{-h}$ and an overall costs of \(\bigO{2^{h} ⋅ c_{n/2,α, q}}\). This improves on exhaustive search, which costs \(\bigO{2^{h} ⋅ \binom{n}{h}}\), when \(c_{{n/2,α, q}} \in  o\left(\binom{n}{h}\right)\).
  
\end{remark}


\section{Combined}\label{sec:combined}

Combining the strategies described in this work, we arrive at Algorithm~\ref{alg:complete} (\MYALG). It takes a flag \(sparse\) which enables the sparse strategy of Algorithm~\ref{alg:sparse}. In this case, we enforce that distinguishing LWE from uniform succeeds with probability \(1-2^{-κ}\) when we guessed \(\s'\) correctly. Clearly, this parameter can be improved, i.e.~this probability reduced, but amplifying the success probability is relatively cheap, so we forego this improvement.

\begin{algorithm}
  \KwData{candidate LWE samples \(\mat{A},\vec{c} \in \ZZq^{m × n}  × \ZZq^{m}\)}
  \KwData{BKZ block sizes \(β, β' ≥ 2\)}
  \KwData{target success probability \(ε\)}
  \KwData{\(sparse\) flag toggling sparse strategy}
  \KwData{scale factor \(c≥ 1\)}
  \KwData{dimension parameter \(0 \leq k \leq n\), \(0\) when \(sparse\) is set}
  \KwData{density parameter \(0 ≤ ℓ ≤ k\), \(0\) when \(sparse\) is set}
  \SetKwFor{MRepeat}{repeat}{}{}
  \tcp{distinguishing advantage per sample from \(β, β'\)}
  \(ε_{d} \gets \exp(-π{(\E{\vecnorm{\vec{y}_{i}}}⋅α)}^{2})\)\; 
  \eIf{\(sparse\)}{
    \(ε_{t} \gets 1-1/2^{κ} \); \tcp{for  security parameter \(κ\)}
    \(r \gets \max\left(\lceil\log(1-ε)/\log(1 - {\sum_{j=0}^{ℓ} p_{k,j}})\rceil,1 \right)\)\;
  }{
    \(ε_{t}, r \gets ε, 1\)\;
  }
  \tcp{required number of samples for majority vote}
  \(m \gets \lceil2\,\log(2 - 2\,ε_{t})/\log(1 - 4\, ε_{d}^{2})\rceil\); 
 
  \MRepeat{\(r\) times}{
    \(\mat{P} \sample\) \(n \times n\) permutation matrices\;
    \([\mat{A}_{0} \mid \mat{A}_{1}] \gets \mat{A} ⋅ \mat{P}\) with \(\mat{A}_{0} \in \ZZ_{q}^{m \times (n-k)}\)\;
    \(\mat{L} \gets\) basis for \(\{(\vec{y}, \vec{x}/c) \in \ZZ^m × {({1}/{c} ⋅ \ZZ)}^n : \vec{y} ⋅ \mat{A}_{0} ≡ \vec{x} \bmod q\}\)\;
    \(\mat{L}' \gets\) BKZ-\(β\) reduced basis for \(\mat{L}\)\;
    \For{\(i \gets 0\) \KwTo{} \(m-1\)}{
      \(\mat{U} \sample \) a sparse unimodular matrix with small entries\;
      \(\mat{L}_{i} \gets \) \(\mat{U} ⋅ \mat{L}'\)\;
      \(\mat{L}'_{i} \gets \) BKZ-\(β'\) reduced basis for \(\mat{L}_{i}\)\;
      \((\vec{w}_{i},\vec{v}_{i}) \gets\) shortest row vector in \(\mat{L}'_{i}\)\;
      \(e'_{i} \gets \ip{\vec{w}_{i}}{\vec{c}}\)\;      
    }
    \If{\(e'_{i}\) follow discrete Gaussian distribution}{
      \Return\(\top\)\;}
    \ForEach{\(\s'\) in the set of \(\sum_{i=1}^{ℓ} \binom{k}{i} ⋅ 2^i\) candidate solutions}{
      \For{\(i \gets 0\) \KwTo{} \(m-1\)}{
        \(e''_{i} = e'_{i} + \ip{\vec{w}_{i} \cdot \mat{A}_{1}}{\s'}\)\;
      }
      \If{\(e''_{i}\) follow discrete Gaussian distribution}{\Return\(\top\)\;}      
    }
  }
  \Return\(\bot\)\;
 \caption{\small \MYALG\@: (Sparse) BKW-style SIS Strategy for solving LWE}\label{alg:complete}
\end{algorithm}

We give an implementation of Algorithm~\ref{alg:complete} for \(sparse=\textnormal{false}\) in Appendix~\ref{sec:implementation}. For brevity, we skip the \(sparse=\textnormal{true}\) case. We also tested our implementation on several parameter sets:\footnote{All experiments on ``strombenzin'' with Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz.}

\begin{enumerate}
\item Considering an LWE instance with \(n=100\) and \(q≈ 2^{23}\), \(α = 8/q\) and \(h=20\), we first BKZ-50 reduced the basis \(\mat{L}\) for \(c=16\). This produced a short vector \(\vec{w}\) such that \(\abs{\ip{\vec{w}}{\vec{c}}} ≈ 2^{15.3}\). Then, running LLL 256 times, we produced short vectors such that \(\E{\abs{\ip{\vec{w}_{i}}{\vec{c}}}} = 2^{15.7}\) and standard deviation \(2^{16.6}\).

\item Considering an LWE instance with \(n=140\) and \(q≈ 2^{40}\), \(α = 8/q\) and \(h=32\), we first BKZ-70 reduced the basis \(\mat{L}\) for \(c=1\). This took 64 hours and produced a short vector \(\vec{w}\) such that \(\abs{\ip{\vec{w}}{\vec{c}}} ≈ 2^{23.7}\), with \(\E{\abs{\ip{\vec{w}}{\vec{c}}}} ≈ 2^{25.5}\) conditioned on \(\abs{\vec{w}}\). Then, running LLL 140 times (each run taking about 50 seconds on average), we produced short vectors such that \(\E{\abs{\ip{\vec{w}_{i}}{\vec{c}}}} = 2^{26.0}\) and standard deviation \(2^{26.4}\) for \(\ip{\vec{w}_{i}}{\vec{c}}\).

\item Considering the same LWE instance with \(n=140\) and \(q≈ 2^{40}\), \(α = 8/q\) and \(h=32\), we first BKZ-70 reduced the basis \(\mat{L}\) for \(c=16\). This took 65 hours and produced a short vector \(\vec{w}\) such that \(\abs{\ip{\vec{w}}{\vec{c}}} ≈ 2^{24.7}\) after scaling by \(c\), cf. \(\E{\abs{\ip{\vec{w}}{\vec{c}}}} ≈ 2^{24.8}\). Then, running LLL 140 times (each run taking about 50 seconds on average), we produced short vectors such that \(\E{\abs{\ip{\vec{w}_{i}}{\vec{c}}}} = 2^{25.5}\) and standard deviation \(2^{25.9}\) for \(\ip{\vec{w}_{i}}{\vec{c}}\).

\item Considering again the same LWE instance with \(n=140\) and \(q≈ 2^{40}\), \(α = 8/q\) and \(h=32\), we first BKZ-70 reduced the basis \(\mat{L}\) for \(c=1\). This took 30 hours and produced a short vector \(\vec{w}\) such that \(\abs{\ip{\vec{w}}{\vec{c}}} ≈ 2^{25.2}\), cf. \(\E{\abs{\ip{\vec{w}}{\vec{c}}}} ≈ 2^{25.6}\). Then, running LLL 1024 times (each run taking about 50 seconds on average), we produced 1016 short vectors such that \(\E{\abs{\ip{\vec{w}_{i}}{\vec{c}}}} = 2^{25.8}\) and standard deviation \(2^{26.1}\) for \(\ip{\vec{w}_{i}}{\vec{c}}\).

\item Considering an LWE instance with \(n=180\) and \(q≈ 2^{40}\), \(α = 8/q\) and \(h=48\), we first BKZ-70 reduced the basis \(\mat{L}\) for \(c=8\). This took 198 hours\footnote{We ran 49 BKZ tours until fplll's auto abort triggered. After 16 tours the norm of the then shortest vector was by a factor 1.266 larger than the norm of the shortest vector found after 49 tours.} and produced a short vector \(\vec{w}\) such that \(\abs{\ip{\vec{w}}{\vec{c}}} ≈ 2^{26.7}\), cf. \(\E{\abs{\ip{\vec{w}}{\vec{c}}}} ≈ 2^{25.9}\). Then, running LLL 180 times (each run taking about 500 seconds on average), we produced short vectors such that \(\E{\abs{\ip{\vec{w}_{i}}{\vec{c}}}} = 2^{26.6}\) and standard deviation \(2^{26.9}\) for \(\ip{\vec{w}_{i}}{\vec{c}}\).
  
\end{enumerate}

All our experiments match our prediction bounding the growth of the norms of our vectors by a factor of two. Note, however, that in the fourth experiment 1 in 128 vectors found with LLL was a duplicate of previously discovered vector, indicating that re-randomisation is not perfect. While the effect of this loss on the running time of the overall algorithm is small, it highlights that further research is required on the interplay of re-randomisation and lattice reduction.

Applying Algorithm~\ref{alg:complete} to parameter choices from HElib and SEAL, we arrive at the estimates in Table~\ref{tab:results}. These estimates were produced using the Sage~\cite{sagemath} code available at \url{http://bitbucket.org/malb/lwe-estimator} which optimises the parameters \(c, ℓ, k, β\) to minimise the overall cost.

For the HElib parameters in Table~\ref{tab:results} we chose the sparse strategy. Here, amortising costs as in Section~\ref{sec:amortising-costs} did not lead to a significant improvement, which is why we did not use it in these cases. All considered lattices have dimension $<2\,n$. Hence, one Ring-LWE sample is sufficient to mount these attacks. Note that this is less than the dual attack as described in~\cite{C:GenHalSma12} would require (two samples).

For the SEAL parameter choices in Table~\ref{tab:results}, dimension $n=1024$  requires two Ring-LWE samples, larger dimensions only require one sample. Here, amortising costs as in Algorithm~\ref{alg:amortising} does lead to a modest improvement and is hence enabled.

Finally, we note that reducing \(q\) to \(\approx 2^{34}\) resp.\ \(\approx 2^{560}\) leads to an estimated cost of 80 bits for \(n=1024\) resp.\ \(n=16384\) for \(\s \sample \B[64]{-}\). For \(\s \sample \B{-}\), \(q \approx 2^{{40}}\) resp.\ \(q \approx 2^{660}\) leads to an estimated cost of 80 bits under the techniques described here. In both cases, we assume \(\sigma \approx 3.2\).

\anonymous{}{
\subsubsection{Acknowledgements.} We thank Kenny Paterson and Adeline Roux-Langlois for helpful comments on an earlier draft of this work. We thank Hao Chen for reporting an error in an earlier version of this work.
}

\clearpage
\bibliographystyle{alpha}
\bibliography{abbrev3,crypto_crossref,local}

\appendix

\section{Rerandomisation}\label{sec:rerandomisation}

\begin{algorithm}[H]
  \SetAlgoLined{}
  \KwData{\(n × m\) matrix \mat{L}}
  \KwData{density parameter \(d\), default \(d=3\)}
  \KwResult{\(\mat{U} ⋅ \mat{L}\) where \(\mat{U}\) is a sparse, unimodular matrix.}
  \For{\(i \leftarrow 0\) \KwTo{} \(4⋅n -1\)}{
    \(a \sample \{0, n-1\}\)\;
    \(b \sample \{0, n-1\}\setminus\{a\}\)\;
    \(\mat{L}_{(b)}, \mat{L}_{(a)} \leftarrow \mat{L}_{(a)}, \mat{L}_{(b)}\) \;
  }
  \For{\(a \leftarrow 0 \) \KwTo{} \(n-2\)}{
    \For{\(i \leftarrow 0\) \KwTo{} \(d-1\)}{
      \(b \sample \{a+1, n-1\}\)\;
      \(s \sample \{0, 1\}\)\;
      \(\mat{L}_{(a)} \leftarrow \mat{L}_{(a)} + {(-1)}^{s} ⋅ \mat{L}_{(b)}\)\;
    }   
  }
  \Return{\(\mat{L}\)}\;
\caption{Rerandomisation strategy in the fplll library~\cite{fplll}.}\label{alg:rerandomisation}
\end{algorithm}

\section{Implementation}\label{sec:implementation}

\lstinputlisting[language=Python,basicstyle=\tt\tiny\relax]{experiment.py}

\clearpage
\section{Alternative Cost Models}\label{sec:alternative-cost-models}
\begin{table}[h]
  \scriptsize
  \begin{center}
    \begin{tabular}{rrrrrr}      
      \multicolumn{6}{c}{\cite{RSA:LinPei11}}\\
      \midrule
      \(n\) & 1024 & 2048 & 4096 & 8192 & 16384\\
      \midrule
      \multicolumn{6}{c}{SEAL~v2.0 80-bit}\\
      $q$   & 47.5  &   95.4 &  192.0 &  392.1 & 799.6\\
      dual  & 107.9 &   97.4 &   88.0 &   82.0 &  78.8\\
      small & 80.5  &   81.1 &   78.2 &   76.4 &  75.9\\
      \midrule
      \multicolumn{6}{c}{HElib 80-bit}\\
      $q$    &  47.0 &   87.0 &  167.0 &  326.0 &  638.0\\
      dual   & 111.5 &  112.4 &  111.5 &  111.2 &  111.2\\
      sparse & 58.1  &   62.6 &   65.4 &   69.2 &   71.5\\
      \midrule
      \multicolumn{6}{c}{HElib 128-bit}\\
      $q$    & 38.0  &   70.0 &  134.0 &  261.0 &  511.0\\
      dual   & 162.0 &  162.1 &  160.1 &  159.1 &  159.5\\
      sparse & 76.3  &   81.9 &   85.8 &   86.2 &   90.3\\
      \midrule
      \multicolumn{6}{c}{\cite{AFRICACRYPT:LepNae14,JMC:AlbPlaSco15}, \(8-16\) BKZ tours}\\
      \midrule
      \(n\) & 1024 & 2048 & 4096 & 8192 & 16384\\
      \midrule
      \multicolumn{6}{c}{SEAL~v2.0 80-bit}\\
      $q$   & 47.5  &   95.4 &  192.0 &  392.1 & 799.6\\
      dual  & 101.2 &   91.7 &   83.1 &   78.3 &   76.1\\
      small & 74.5  &   76.0 &   74.1 &   73.5 &   73.2 \\
      \midrule
      \multicolumn{6}{c}{HElib 80-bit}\\
      $q$    &  47.0 &   87.0 &  167.0 &  326.0 &  638.0\\
      dual   & 105.1 &  107.1 &  106.8 &  107.7 &  108.8\\
      sparse &  54.1 &   59.1 &   62.8 &   65.8 &   68.9\\
      \midrule
      \multicolumn{6}{c}{HElib 128-bit}\\
      $q$    & 38.0  &   70.0 &  134.0 &  261.0 &  511.0\\
      dual   & 158.4 &  159.8 &  158.6 &  158.3 &  160.0\\
      sparse & 72.0 &   77.4 &   81.4 &   84.3 &   87.1\\
    \end{tabular}
  \end{center}
  \caption{Costs of dual attacks on HElib and SEAL~v2.0 in the~\cite{RSA:LinPei11} cost model resp.\ assuming SVP in dimension \(\beta\) costs \(2^{0.64\beta -28}\) operations as in~\cite{AFRICACRYPT:LepNae14} plugged into the estimator from~\cite{JMC:AlbPlaSco15}; cf.~Table~\ref{tab:results}.}\label{tab:results-alt}
\end{table}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
%%% chktex 17
